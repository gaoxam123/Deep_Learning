{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import string\n",
    "import random\n",
    "import sys\n",
    "import unidecode\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)\n",
    "\n",
    "file = unidecode.unidecode(open('shakespeare_larger.txt').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embed = nn.Embedding(input_size, hidden_size)\n",
    "        self.rnn = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        out = self.embed(x)\n",
    "        out, (hidden, cell) = self.rnn(out.unsqueeze(1), (hidden, cell))\n",
    "        out = self.fc(out.reshape(out.shape[0], -1))\n",
    "\n",
    "        return out, hidden, cell\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "        cell = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "\n",
    "        return hidden, cell\n",
    "    \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.chunk_len = 250\n",
    "        self.num_epochs = 10000\n",
    "        self.batch_size = 1\n",
    "        self.print_every = 50\n",
    "        self.hidden_size = 256\n",
    "        self.num_layers = 2\n",
    "        self.learning_rate = 0.003\n",
    "\n",
    "    def char_tensor(self, string):\n",
    "        tensor = torch.zeros(len(string)).long()\n",
    "        for c in range(len(string)):\n",
    "            tensor[c] = all_characters.index(string[c])\n",
    "\n",
    "        return tensor\n",
    "\n",
    "    def get_random_batch(self):\n",
    "        start_idx = random.randint(0, len(file) - self.chunk_len)\n",
    "        end_idx = start_idx + self.chunk_len + 1\n",
    "        text_string = file[start_idx:end_idx]\n",
    "        text_input = torch.zeros(self.batch_size, self.chunk_len)\n",
    "        text_target = torch.zeros(self.batch_size, self.chunk_len)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            text_input[i, :] = self.char_tensor(text_string[:-1])\n",
    "            text_target[i, :] = self.char_tensor(text_string[1:])\n",
    "\n",
    "        return text_input.long(), text_target.long()\n",
    "\n",
    "    def generate(self, initial_string='A', prediction_len=100, temperature=0.85):\n",
    "        hidden, cell = self.rnn.init_hidden(self.batch_size) #########\n",
    "        initial_input = self.char_tensor(initial_string)\n",
    "        predicted = initial_string\n",
    "\n",
    "        for p in range(len(initial_string) - 1):\n",
    "            out, hidden, cell = self.rnn(initial_input[p].view(1).to(device), hidden, cell) #######\n",
    "\n",
    "        last_char = initial_input[-1]\n",
    "\n",
    "        for p in range(prediction_len):\n",
    "            out, hidden, cell = self.rnn(last_char.view(1).to(device), hidden, cell) #########\n",
    "            output_dist = out.data.view(-1).div(temperature).exp()\n",
    "            top_char = torch.multinomial(output_dist, 1)[0]\n",
    "            predicted_char = all_characters[top_char]\n",
    "            predicted += predicted_char\n",
    "            last_char = self.char_tensor(predicted_char)\n",
    "\n",
    "        return predicted\n",
    "\n",
    "    def train(self):\n",
    "        self.rnn = RNN(n_characters, self.hidden_size, self.num_layers, n_characters).to(device)\n",
    "        optimizer = torch.optim.Adam(self.rnn.parameters(), self.learning_rate)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        for epoch in tqdm(range(self.num_epochs)):\n",
    "            input, target = self.get_random_batch()\n",
    "            input, target = input.to(device), target.to(device)\n",
    "\n",
    "            hidden, cell = self.rnn.init_hidden(self.batch_size)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = 0\n",
    "\n",
    "            for c in range(self.chunk_len):\n",
    "                output, hidden, cell = self.rnn(input[:, c], hidden, cell)\n",
    "                loss += loss_fn(output, target[:, c])\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss = loss.item() / self.chunk_len\n",
    "\n",
    "            if epoch % self.print_every == 0:\n",
    "                print(f\"loss: {loss} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Generator:\n\tUnexpected key(s) in state_dict: \"rnn.embed.weight\", \"rnn.rnn.weight_ih_l0\", \"rnn.rnn.weight_hh_l0\", \"rnn.rnn.bias_ih_l0\", \"rnn.rnn.bias_hh_l0\", \"rnn.rnn.weight_ih_l1\", \"rnn.rnn.weight_hh_l1\", \"rnn.rnn.bias_ih_l1\", \"rnn.rnn.bias_hh_l1\", \"rnn.fc.weight\", \"rnn.fc.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m gen\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\minht\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2152\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2147\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2148\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2149\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2153\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Generator:\n\tUnexpected key(s) in state_dict: \"rnn.embed.weight\", \"rnn.rnn.weight_ih_l0\", \"rnn.rnn.weight_hh_l0\", \"rnn.rnn.bias_ih_l0\", \"rnn.rnn.bias_hh_l0\", \"rnn.rnn.weight_ih_l1\", \"rnn.rnn.weight_hh_l1\", \"rnn.rnn.bias_ih_l1\", \"rnn.rnn.bias_hh_l1\", \"rnn.fc.weight\", \"rnn.fc.bias\". "
     ]
    }
   ],
   "source": [
    "gen.load_state_dict(torch.load('models/model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d3f38fca4b24430a730a7fff72a43c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 4.61071337890625 \n",
      "\n",
      "loss: 2.510514892578125 \n",
      "\n",
      "loss: 2.3075009765625 \n",
      "\n",
      "loss: 2.18582763671875 \n",
      "\n",
      "loss: 2.241069091796875 \n",
      "\n",
      "loss: 1.9728653564453125 \n",
      "\n",
      "loss: 2.106625732421875 \n",
      "\n",
      "loss: 1.97143359375 \n",
      "\n",
      "loss: 2.166918212890625 \n",
      "\n",
      "loss: 1.9970152587890626 \n",
      "\n",
      "loss: 2.058250244140625 \n",
      "\n",
      "loss: 1.81194970703125 \n",
      "\n",
      "loss: 1.98783642578125 \n",
      "\n",
      "loss: 1.925274658203125 \n",
      "\n",
      "loss: 2.01877978515625 \n",
      "\n",
      "loss: 1.816458984375 \n",
      "\n",
      "loss: 1.693320068359375 \n",
      "\n",
      "loss: 1.7893072509765624 \n",
      "\n",
      "loss: 1.8161129150390625 \n",
      "\n",
      "loss: 1.92575537109375 \n",
      "\n",
      "loss: 1.9623277587890624 \n",
      "\n",
      "loss: 1.9824852294921875 \n",
      "\n",
      "loss: 1.7685836181640624 \n",
      "\n",
      "loss: 1.5866041259765624 \n",
      "\n",
      "loss: 1.703200927734375 \n",
      "\n",
      "loss: 1.86522509765625 \n",
      "\n",
      "loss: 1.8189678955078126 \n",
      "\n",
      "loss: 1.6810771484375 \n",
      "\n",
      "loss: 1.7042664794921876 \n",
      "\n",
      "loss: 1.789553466796875 \n",
      "\n",
      "loss: 1.537436279296875 \n",
      "\n",
      "loss: 1.9293896484375 \n",
      "\n",
      "loss: 2.051091064453125 \n",
      "\n",
      "loss: 1.65183349609375 \n",
      "\n",
      "loss: 1.7150777587890624 \n",
      "\n",
      "loss: 1.78080615234375 \n",
      "\n",
      "loss: 1.679027099609375 \n",
      "\n",
      "loss: 1.7594903564453126 \n",
      "\n",
      "loss: 1.4643525390625 \n",
      "\n",
      "loss: 1.6589324951171875 \n",
      "\n",
      "loss: 1.759664794921875 \n",
      "\n",
      "loss: 1.6832821044921875 \n",
      "\n",
      "loss: 1.8421202392578124 \n",
      "\n",
      "loss: 1.697014892578125 \n",
      "\n",
      "loss: 1.4877508544921876 \n",
      "\n",
      "loss: 1.8349173583984375 \n",
      "\n",
      "loss: 1.5605059814453126 \n",
      "\n",
      "loss: 1.583270263671875 \n",
      "\n",
      "loss: 1.72205859375 \n",
      "\n",
      "loss: 1.462387451171875 \n",
      "\n",
      "loss: 1.891309326171875 \n",
      "\n",
      "loss: 1.5307127685546875 \n",
      "\n",
      "loss: 1.5025374755859375 \n",
      "\n",
      "loss: 1.53658056640625 \n",
      "\n",
      "loss: 1.62462548828125 \n",
      "\n",
      "loss: 1.7707584228515625 \n",
      "\n",
      "loss: 1.669788330078125 \n",
      "\n",
      "loss: 1.6974093017578125 \n",
      "\n",
      "loss: 1.393955322265625 \n",
      "\n",
      "loss: 1.805260009765625 \n",
      "\n",
      "loss: 1.424548095703125 \n",
      "\n",
      "loss: 1.52701123046875 \n",
      "\n",
      "loss: 1.7051058349609376 \n",
      "\n",
      "loss: 1.7565230712890625 \n",
      "\n",
      "loss: 1.8133045654296875 \n",
      "\n",
      "loss: 1.766792236328125 \n",
      "\n",
      "loss: 1.49344091796875 \n",
      "\n",
      "loss: 1.5245411376953124 \n",
      "\n",
      "loss: 1.5966868896484374 \n",
      "\n",
      "loss: 1.7109381103515624 \n",
      "\n",
      "loss: 1.618157470703125 \n",
      "\n",
      "loss: 1.6782628173828125 \n",
      "\n",
      "loss: 1.4949775390625 \n",
      "\n",
      "loss: 1.5372828369140625 \n",
      "\n",
      "loss: 1.848304931640625 \n",
      "\n",
      "loss: 1.5260048828125 \n",
      "\n",
      "loss: 1.5650870361328124 \n",
      "\n",
      "loss: 1.52058642578125 \n",
      "\n",
      "loss: 1.49360107421875 \n",
      "\n",
      "loss: 1.6540206298828124 \n",
      "\n",
      "loss: 1.5825899658203124 \n",
      "\n",
      "loss: 1.8887137451171876 \n",
      "\n",
      "loss: 1.7304276123046876 \n",
      "\n",
      "loss: 1.6852584228515626 \n",
      "\n",
      "loss: 1.5996392822265626 \n",
      "\n",
      "loss: 1.7784625244140626 \n",
      "\n",
      "loss: 1.64017431640625 \n",
      "\n",
      "loss: 1.8027093505859375 \n",
      "\n",
      "loss: 1.5336275634765626 \n",
      "\n",
      "loss: 1.5305040283203124 \n",
      "\n",
      "loss: 1.3181492919921876 \n",
      "\n",
      "loss: 1.616659423828125 \n",
      "\n",
      "loss: 1.7434571533203125 \n",
      "\n",
      "loss: 1.57225732421875 \n",
      "\n",
      "loss: 1.558883056640625 \n",
      "\n",
      "loss: 1.5566654052734374 \n",
      "\n",
      "loss: 1.3632738037109375 \n",
      "\n",
      "loss: 1.4478939208984376 \n",
      "\n",
      "loss: 1.532974609375 \n",
      "\n",
      "loss: 1.5098792724609376 \n",
      "\n",
      "loss: 1.5432950439453126 \n",
      "\n",
      "loss: 1.544808349609375 \n",
      "\n",
      "loss: 1.646124755859375 \n",
      "\n",
      "loss: 1.425646240234375 \n",
      "\n",
      "loss: 1.6766986083984374 \n",
      "\n",
      "loss: 1.9247802734375 \n",
      "\n",
      "loss: 1.3834002685546876 \n",
      "\n",
      "loss: 2.82366015625 \n",
      "\n",
      "loss: 1.2809095458984374 \n",
      "\n",
      "loss: 1.38536474609375 \n",
      "\n",
      "loss: 1.4151435546875 \n",
      "\n",
      "loss: 1.57365234375 \n",
      "\n",
      "loss: 1.8027357177734376 \n",
      "\n",
      "loss: 1.652334228515625 \n",
      "\n",
      "loss: 1.3144830322265626 \n",
      "\n",
      "loss: 1.7078577880859376 \n",
      "\n",
      "loss: 1.5145537109375 \n",
      "\n",
      "loss: 1.56746728515625 \n",
      "\n",
      "loss: 1.546034423828125 \n",
      "\n",
      "loss: 1.51404736328125 \n",
      "\n",
      "loss: 1.393353271484375 \n",
      "\n",
      "loss: 1.589005859375 \n",
      "\n",
      "loss: 1.60520068359375 \n",
      "\n",
      "loss: 1.6688125 \n",
      "\n",
      "loss: 1.4327698974609375 \n",
      "\n",
      "loss: 1.6359954833984376 \n",
      "\n",
      "loss: 1.7528001708984375 \n",
      "\n",
      "loss: 1.632171875 \n",
      "\n",
      "loss: 1.5046700439453125 \n",
      "\n",
      "loss: 1.682285400390625 \n",
      "\n",
      "loss: 1.456971923828125 \n",
      "\n",
      "loss: 1.6926376953125 \n",
      "\n",
      "loss: 1.374447509765625 \n",
      "\n",
      "loss: 1.5429315185546875 \n",
      "\n",
      "loss: 1.7463779296875 \n",
      "\n",
      "loss: 1.2819136962890625 \n",
      "\n",
      "loss: 1.5325223388671876 \n",
      "\n",
      "loss: 1.603477783203125 \n",
      "\n",
      "loss: 1.5129373779296875 \n",
      "\n",
      "loss: 1.5953760986328125 \n",
      "\n",
      "loss: 1.6477325439453125 \n",
      "\n",
      "loss: 1.45555322265625 \n",
      "\n",
      "loss: 1.4201669921875 \n",
      "\n",
      "loss: 1.4834456787109376 \n",
      "\n",
      "loss: 1.6486422119140625 \n",
      "\n",
      "loss: 1.7151942138671874 \n",
      "\n",
      "loss: 1.569617919921875 \n",
      "\n",
      "loss: 1.5122364501953125 \n",
      "\n",
      "loss: 1.554031005859375 \n",
      "\n",
      "loss: 1.55071630859375 \n",
      "\n",
      "loss: 1.5478602294921875 \n",
      "\n",
      "loss: 1.3663804931640624 \n",
      "\n",
      "loss: 1.5235384521484374 \n",
      "\n",
      "loss: 1.3947618408203124 \n",
      "\n",
      "loss: 1.4514940185546874 \n",
      "\n",
      "loss: 1.4560941162109375 \n",
      "\n",
      "loss: 1.5403099365234374 \n",
      "\n",
      "loss: 1.5485111083984375 \n",
      "\n",
      "loss: 1.602212158203125 \n",
      "\n",
      "loss: 1.3758707275390625 \n",
      "\n",
      "loss: 1.5232838134765625 \n",
      "\n",
      "loss: 1.50302783203125 \n",
      "\n",
      "loss: 1.5478326416015624 \n",
      "\n",
      "loss: 1.53011279296875 \n",
      "\n",
      "loss: 1.407794677734375 \n",
      "\n",
      "loss: 1.5089647216796875 \n",
      "\n",
      "loss: 1.8766453857421874 \n",
      "\n",
      "loss: 1.5834283447265625 \n",
      "\n",
      "loss: 1.32700537109375 \n",
      "\n",
      "loss: 1.474374755859375 \n",
      "\n",
      "loss: 1.4112825927734376 \n",
      "\n",
      "loss: 1.50551123046875 \n",
      "\n",
      "loss: 1.5524796142578126 \n",
      "\n",
      "loss: 2.013839599609375 \n",
      "\n",
      "loss: 1.302499267578125 \n",
      "\n",
      "loss: 1.596222412109375 \n",
      "\n",
      "loss: 1.364767333984375 \n",
      "\n",
      "loss: 1.7025546875 \n",
      "\n",
      "loss: 1.75606640625 \n",
      "\n",
      "loss: 1.564107421875 \n",
      "\n",
      "loss: 1.4274952392578124 \n",
      "\n",
      "loss: 1.2296728515625 \n",
      "\n",
      "loss: 1.9181517333984375 \n",
      "\n",
      "loss: 1.5186654052734374 \n",
      "\n",
      "loss: 1.4964283447265625 \n",
      "\n",
      "loss: 1.508864013671875 \n",
      "\n",
      "loss: 1.5212232666015626 \n",
      "\n",
      "loss: 1.3671705322265626 \n",
      "\n",
      "loss: 1.59079150390625 \n",
      "\n",
      "loss: 1.7305736083984375 \n",
      "\n",
      "loss: 1.5239224853515625 \n",
      "\n",
      "loss: 1.5749833984375 \n",
      "\n",
      "loss: 1.365610107421875 \n",
      "\n",
      "loss: 1.7424091796875 \n",
      "\n",
      "loss: 1.373830078125 \n",
      "\n",
      "loss: 1.498778076171875 \n",
      "\n",
      "loss: 1.534112060546875 \n",
      "\n",
      "loss: 1.4212205810546874 \n",
      "\n",
      "loss: 1.31208544921875 \n",
      "\n",
      "loss: 1.5300572509765624 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "gen.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gaure:\\nAnd where he life worship on her banmory\\nTo it awe\\nAs thou fear, night well; a for oness speech\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen.generate('gau')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = Path('models')\n",
    "model_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_name = 'model.pth'\n",
    "model_path_save = model_path/model_name\n",
    "\n",
    "torch.save(obj=gen.state_dict(), f=model_path_save)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
